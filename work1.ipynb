{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "electrode_to_region = {\n",
    "    'Fp1': 'Frontal', 'Fp2': 'Frontal', 'Fpz': 'Frontal', 'AF3': 'Frontal', 'AF4': 'Frontal', \n",
    "    'F11': 'Frontal', 'F7': 'Frontal', 'F5': 'Frontal', 'F3': 'Frontal', 'F1': 'Frontal', \n",
    "    'Fz': 'Frontal', 'F2': 'Frontal', 'F4': 'Frontal', 'F6': 'Frontal', 'F8': 'Frontal', \n",
    "    'F12': 'Frontal', 'FT11': 'Frontal', 'FC5': 'Frontal', 'FC3': 'Frontal', 'FC1': 'Frontal', \n",
    "    'FCz': 'Frontal', 'FC2': 'Frontal', 'FC4': 'Frontal', 'FC6': 'Frontal', 'FT12': 'Frontal',\n",
    "    'C5': 'Central', 'C3': 'Central', 'C1': 'Central', 'Cz': 'Central', 'C2': 'Central', \n",
    "    'C4': 'Central', 'C6': 'Central',\n",
    "    'T7': 'Temporal', 'T8': 'Temporal', 'TP7': 'Temporal', 'TP8': 'Temporal',\n",
    "    'P7': 'Parietal', 'P5': 'Parietal', 'P3': 'Parietal', 'P1': 'Parietal', 'Pz': 'Parietal', \n",
    "    'P2': 'Parietal', 'P4': 'Parietal', 'P6': 'Parietal', 'P8': 'Parietal', 'PO7': 'Parietal', \n",
    "    'PO3': 'Parietal', 'POz': 'Parietal', 'PO4': 'Parietal', 'PO8': 'Parietal',\n",
    "    'O1': 'Occipital', 'Oz': 'Occipital', 'O2': 'Occipital',\n",
    "    'M1': 'Reference', 'M2': 'Reference',  \n",
    "}\n",
    "# 电极位置编码\n",
    "def generate_position_encoding(positions, d=256):\n",
    "    encodings = []\n",
    "    for i, pos in enumerate(positions):\n",
    "        x, y, z = pos\n",
    "        encoding = []\n",
    "        for i in range(d // 2):\n",
    "            encoding.append(np.sin((x / (10000 ** (2 * i / d)))) )\n",
    "            encoding.append(np.cos((x / (10000 ** (2 * i / d)))) )\n",
    "            encoding.append(np.sin((y / (10000 ** (2 * i / d)))) )\n",
    "            encoding.append(np.cos((y / (10000 ** (2 * i / d)))) )\n",
    "            encoding.append(np.sin((z / (10000 ** (2 * i / d)))) )\n",
    "            encoding.append(np.cos((z / (10000 ** (2 * i / d)))) )\n",
    "        encodings.append(encoding)\n",
    "    return np.array(encodings)\n",
    "\n",
    "# EEGNet模型定义\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, num_regions=20, input_dim=256, num_heads=4, num_filters=32):\n",
    "        super(EEGNet, self).__init__()\n",
    "        \n",
    "        # EEGNet特征提取部分\n",
    "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size=(1, 64), padding='same')\n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters * 2, kernel_size=(1, 64), padding='same')\n",
    "        self.conv3 = nn.Conv2d(num_filters * 2, num_filters * 4, kernel_size=(1, 64), padding='same')\n",
    "        self.pool = nn.MaxPool2d((1, 2))\n",
    "        self.fc1 = nn.Linear(num_filters * 4 * 64, input_dim)  # 输出到256维特征空间 4x64\n",
    "        \n",
    "        # 区域解码器\n",
    "        self.region_decoders = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.MaxPool1d(kernel_size=2)\n",
    "            ) for _ in range(num_regions)\n",
    "        ])\n",
    "        \n",
    "        # 动态注意力机制\n",
    "        self.fc_attention = nn.Linear(input_dim, 1)\n",
    "        \n",
    "        # 多头注意力机制\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        \n",
    "        # 最终分类层\n",
    "        self.fc_out = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 第一步：通过EEGNet提取特征\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten 展平\n",
    "        x = self.fc1(x)  # 输出到256维特征空间\n",
    "        \n",
    "        # 第二步：通过区域解码器处理不同区域\n",
    "        region_outputs = []\n",
    "        for decoder in self.region_decoders:\n",
    "            region_outputs.append(decoder(x))\n",
    "        \n",
    "        # 第三步：通过动态注意力机制调整区域重要性\n",
    "        attention_scores = self.fc_attention(x)  # 计算注意力得分\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # 归一化为权重\n",
    "        \n",
    "        # 第四步：通过多头注意力机制整合不同区域的特征\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        # 计算注意力权重并进行加权求和\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        multi_head_output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # 第五步：最终分类\n",
    "        final_output = self.fc_out(multi_head_output)\n",
    "        \n",
    "        return final_output\n",
    "# 处理EEG数据\n",
    "def process_set_files(set_files, words, tmin, tmax, bad_channels=None, filter_params=None, p=0.2):\n",
    "    X_all, y_all = [], []\n",
    "    for set_file in set_files:\n",
    "        print(f\"读取EEGLAB .set 文件: {set_file}...\")\n",
    "\n",
    "        try:\n",
    "            raw = mne.io.read_raw_eeglab(set_file, preload=True)\n",
    "            print(\"文件读取成功！\")\n",
    "        except Exception as e:\n",
    "            print(f\"读取文件时出错: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 手动标记坏道\n",
    "        if bad_channels:\n",
    "            raw.info['bads'] = bad_channels\n",
    "\n",
    "        # 滤波处理\n",
    "        if filter_params:\n",
    "            raw.filter(filter_params['l_freq'], filter_params['h_freq'], fir_design='firwin')\n",
    "\n",
    "        events, current_event_id_map = mne.events_from_annotations(raw)\n",
    "        event_id = {}\n",
    "        event_code_to_word = {}\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            think_code = str(50 + i)  # 只处理“想”相关的事件\n",
    "            think_event_id = current_event_id_map.get(think_code)\n",
    "\n",
    "            if think_event_id is not None:\n",
    "                event_id[f'想_{word}'] = think_event_id\n",
    "                event_code_to_word[think_event_id] = i\n",
    "\n",
    "        epochs = mne.Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax, preload=True, verbose='ERROR', baseline=(0, 0))\n",
    "        X = epochs.get_data()  \n",
    "        y_event_ids = epochs.events[:, 2]\n",
    "\n",
    "        y = []\n",
    "        for label in y_event_ids:\n",
    "            word = event_code_to_word.get(label, -1)\n",
    "            if word != -1:\n",
    "                y.append(word)\n",
    "\n",
    "        y = np.array(y)\n",
    "        valid_indices = (y != -1)\n",
    "        X = X[valid_indices]\n",
    "        y = y[valid_indices]\n",
    "\n",
    "        # 获取电极的三维坐标\n",
    "        ch_pos = np.array([raw.info['chs'][i]['loc'][:3] for i in range(len(raw.ch_names))])\n",
    "        positions = ch_pos[:, :3]  # 只获取x, y, z坐标\n",
    "        position_encodings = generate_position_encoding(positions, d=256)  # 计算位置编码\n",
    "\n",
    "        # 随机掩码\n",
    "        if np.random.rand() < p:\n",
    "            X = np.zeros_like(X)\n",
    "\n",
    "        X_all.append(X)\n",
    "        y_all.append(y)\n",
    "    return np.concatenate(X_all, axis=0), np.concatenate(y_all, axis=0), position_encodings\n",
    "\n",
    "# 数据标准化\n",
    "def standardize_data(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    num_channels = X_train.shape[2]\n",
    "    for i in range(num_channels):\n",
    "        if np.isnan(X_train[:, :, i]).any():\n",
    "            print(f\"Channel {i} contains NaN values, filling with mean.\")\n",
    "            mean_value = np.nanmean(X_train[:, :, i])\n",
    "            X_train[np.isnan(X_train[:, :, i]), :, i] = mean_value  \n",
    "        X_train[:, :, i] = scaler.fit_transform(X_train[:, :, i])\n",
    "        X_test[:, :, i] = scaler.transform(X_test[:, :, i])\n",
    "    return X_train, X_test\n",
    "\n",
    "# EEG 数据集类\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y, position_encodings):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.position_encodings = position_encodings.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.position_encodings[idx]\n",
    "\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    words = [\n",
    "        '农民种菜', '厨师做饭', '祖父喝茶', '病人咳嗽', \n",
    "        '叛军投降', '孔雀开屏', '老牛耕地', '母鸡下蛋', \n",
    "        '蜻蜓点水', '螳螂捕蝉'\n",
    "    ]\n",
    "\n",
    "    # 处理多个训练集和测试集\n",
    "    train_set_files = [\n",
    "        'c:/Users/clock/Desktop/hanzi/008C1.set',\n",
    "    ]\n",
    "    \n",
    "    test_set_files = [\n",
    "        'c:/Users/clock/Desktop/hanzi/008C2.set',\n",
    "    ]\n",
    "\n",
    "    # 坏道通道\n",
    "    bad_channels = ['VEOG', 'HEOG', 'Trigger'] \n",
    "    # 滤波参数（低频和高频）\n",
    "    filter_params = {'l_freq': 1.0, 'h_freq': 80.0}\n",
    "\n",
    "    # 读取数据并处理\n",
    "    X_train, y_train, position_encodings_train = process_set_files(train_set_files, words, tmin=0, tmax=4, bad_channels=bad_channels, filter_params=filter_params)\n",
    "    X_test, y_test, position_encodings_test = process_set_files(test_set_files, words, tmin=0, tmax=4, bad_channels=bad_channels, filter_params=filter_params)\n",
    "\n",
    "    # 标准化数据\n",
    "    X_train, X_test = standardize_data(X_train, X_test)\n",
    "\n",
    "    # 转换为Tensor\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_train = torch.tensor(y_train).to(device)\n",
    "    y_test = torch.tensor(y_test).to(device)\n",
    "    position_encodings_train = torch.tensor(position_encodings_train).to(device)\n",
    "    position_encodings_test = torch.tensor(position_encodings_test).to(device)\n",
    "\n",
    "    # 创建数据集和数据加载器\n",
    "    train_dataset = EEGDataset(X_train, y_train, position_encodings_train)\n",
    "    test_dataset = EEGDataset(X_test, y_test, position_encodings_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # 模型定义\n",
    "    model = EEGNet(num_classes=10, num_channels=X_train.shape[2]).to(device)\n",
    "\n",
    "    # 损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # 训练模型\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels, pos_encodings) in enumerate(train_loader):\n",
    "            inputs, labels, pos_encodings = inputs.to(device), labels.to(device), pos_encodings.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, pos_encodings)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    # 测试模型\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, pos_encodings in test_loader:\n",
    "            inputs, labels, pos_encodings = inputs.to(device), labels.to(device), pos_encodings.to(device)\n",
    "            outputs = model(inputs, pos_encodings)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 输出分类报告\n",
    "    print(classification_report(all_labels, all_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
